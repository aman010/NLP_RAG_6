{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7b670c-beec-48b3-b77c-e421c469a54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain==0.1.0\n",
      "  Using cached langchain-0.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.0) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.local/lib/python3.12/site-packages (from langchain==0.1.0) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.local/lib/python3.12/site-packages (from langchain==0.1.0) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.0) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in ./.local/lib/python3.12/site-packages (from langchain==0.1.0) (0.0.18)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in ./.local/lib/python3.12/site-packages (from langchain==0.1.0) (0.1.19)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in ./.local/lib/python3.12/site-packages (from langchain==0.1.0) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.local/lib/python3.12/site-packages (from langchain==0.1.0) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.0) (2.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.local/lib/python3.12/site-packages (from langchain==0.1.0) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.0) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/tljh/user/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./.local/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.7->langchain==0.1.0) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./.local/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.7->langchain==0.1.0) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.1.0) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.0) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.0) (3.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/tljh/user/lib/python3.12/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.local/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.0) (1.0.0)\n",
      "Using cached langchain-0.1.0-py3-none-any.whl (797 kB)\n",
      "Installing collected packages: langchain\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.2\n",
      "    Uninstalling langchain-0.1.2:\n",
      "      Successfully uninstalled langchain-0.1.2\n",
      "\u001b[33m  WARNING: The script langchain-server is installed in '/home/jupyter-st125490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "Successfully installed langchain-0.1.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate==0.25.0 in ./.local/lib/python3.12/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.12/site-packages (from accelerate==0.25.0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.12/site-packages (from accelerate==0.25.0) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/tljh/user/lib/python3.12/site-packages (from accelerate==0.25.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/tljh/user/lib/python3.12/site-packages (from accelerate==0.25.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.12/site-packages (from accelerate==0.25.0) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.12/site-packages (from accelerate==0.25.0) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.12/site-packages (from accelerate==0.25.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/tljh/user/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (74.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.25.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub->accelerate==0.25.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub->accelerate==0.25.0) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests->huggingface-hub->accelerate==0.25.0) (2024.12.14)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers==4.36.2 in ./.local/lib/python3.12/site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from transformers==4.36.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.local/lib/python3.12/site-packages (from transformers==4.36.2) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.12/site-packages (from transformers==4.36.2) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.12/site-packages (from transformers==4.36.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.12/site-packages (from transformers==4.36.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.12/site-packages (from transformers==4.36.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.12/site-packages (from transformers==4.36.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.12/site-packages (from transformers==4.36.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.12/site-packages (from transformers==4.36.2) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tljh/user/lib/python3.12/site-packages (from transformers==4.36.2) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers==4.36.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers==4.36.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers==4.36.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests->transformers==4.36.2) (2024.12.14)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.12/site-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in ./.local/lib/python3.12/site-packages (from bitsandbytes) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.12/site-packages (from bitsandbytes) (1.26.3)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/tljh/user/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (74.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.12/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in ./.local/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/tljh/user/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (2.6.0)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (0.21.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (1.6.1)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (1.13.1)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (3.9.1)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.local/lib/python3.12/site-packages (from sentence-transformers==2.2.2) (0.29.3)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.12/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.local/lib/python3.12/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/tljh/user/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (74.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.12/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.5.2)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.12/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.12/site-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.12/site-packages (from torchvision->sentence-transformers==2.2.2) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.12.14)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: InstructorEmbedding==1.0.1 in ./.local/lib/python3.12/site-packages (1.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymupdf==1.23.8 in ./.local/lib/python3.12/site-packages (1.23.8)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.7 in ./.local/lib/python3.12/site-packages (from pymupdf==1.23.8) (1.23.7)\n"
     ]
    }
   ],
   "source": [
    "# #langchain library\n",
    "# Install langchain (version compatible with other libraries)\n",
    "!pip install langchain==0.1.0\n",
    "\n",
    "# Install LLM-related packages\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install transformers==4.36.2\n",
    "!pip install bitsandbytes\n",
    "\n",
    "# Install Text Embedding libraries\n",
    "!pip install sentence-transformers==2.2.2\n",
    "!pip install InstructorEmbedding==1.0.1\n",
    "\n",
    "# Install vectorstore-related packages\n",
    "!pip install pymupdf==1.23.8\n",
    "\n",
    "# If you need GPU support with FAISS, try this:\n",
    "# !pip install faiss-gpu-cu12 # CUDA 12.x, Python 3.8+\n",
    "# If you're having trouble with `faiss-gpu`, use the CPU version:\n",
    "# pip install faiss-cpu==1.7.2\n",
    "\n",
    "# !pip install faiss-cpu==1.7.4\n",
    "# conda install -c pytorch faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed237a9b-3ace-4f46-9325-0b6091585edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152220b5-a7a4-49e4-a41c-ff57eef96409",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e593364f-864c-419f-a8dd-0e345f1deeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='You are a friendly assistant knowledgeable in solar energy. Below is a summary of key details from the article:\\n\\n**Article Title**: Photovoltaic Solar Energy: Principles and Applications\\n**Source**: National Renewable Energy Laboratory (NREL)\\n**Key Concepts**:\\n- Photovoltaic (PV) solar energy involves converting sunlight directly into electricity using solar panels.\\n- PV systems are widely used for residential and commercial electricity generation.\\n- The efficiency of PV cells has improved significantly, making them more affordable and efficient.\\n- Solar energy is a renewable and clean energy source, reducing reliance on fossil fuels.\\n\\nPlease answer the following question using this information, in a professional, gentle, and informative manner.\\n\\nQuestion: {question}\\nAnswer:')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a template to answer questions about solar energy based on a single article\n",
    "solar_energy_prompt_template = \"\"\"\n",
    "You are a friendly assistant knowledgeable in solar energy. Below is a summary of key details from the article:\n",
    "\n",
    "**Article Title**: Photovoltaic Solar Energy: Principles and Applications\n",
    "**Source**: National Renewable Energy Laboratory (NREL)\n",
    "**Key Concepts**:\n",
    "- Photovoltaic (PV) solar energy involves converting sunlight directly into electricity using solar panels.\n",
    "- PV systems are widely used for residential and commercial electricity generation.\n",
    "- The efficiency of PV cells has improved significantly, making them more affordable and efficient.\n",
    "- Solar energy is a renewable and clean energy source, reducing reliance on fossil fuels.\n",
    "\n",
    "Please answer the following question using this information, in a professional, gentle, and informative manner.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    template = solar_energy_prompt_template\n",
    ")\n",
    "\n",
    "PROMPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c29b9e-e4f8-4f41-b46e-bf0470a6a6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a friendly assistant knowledgeable in solar energy. Below is a summary of key details from the article:\\n\\n**Article Title**: Photovoltaic Solar Energy: Principles and Applications\\n**Source**: National Renewable Energy Laboratory (NREL)\\n**Key Concepts**:\\n- Photovoltaic (PV) solar energy involves converting sunlight directly into electricity using solar panels.\\n- PV systems are widely used for residential and commercial electricity generation.\\n- The efficiency of PV cells has improved significantly, making them more affordable and efficient.\\n- Solar energy is a renewable and clean energy source, reducing reliance on fossil fuels.\\n\\nPlease answer the following question using this information, in a professional, gentle, and informative manner.\\n\\nQuestion: What is solar energy\\nAnswer:'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT.format(\n",
    "    context = \"Solar energy is the radiant energy from the Sun's light and heat, which can be harnessed using a range of technologies such as solar electricity, solar thermal energy and solar architecture\" ,\n",
    "    question = \"What is solar energy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a9b226-f24b-41b5-9ac4-58c2afa63eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 4\n",
      "page_content=\"Solar energy is the radiant energy from the Sun's light and heat, which can be harnessed using a range of technologies such as solar electricity, \\nsolar thermal energy (including solar water heating) and solar architecture. It is an essential source of renewable energy, and its technologies \\nare broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into \\nsolar power.\" metadata={'source': 'About_solar.pdf', 'file_path': 'About_solar.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025650Z', 'modDate': '', 'trapped': ''}\n",
      "page_content='solar energy, radiation from the Sun capable of producing heat, causing chemical\\nreactions, or generating electricity. The total amount of solar energy incident\\non Earth is vastly in excess of the world’s current and anticipated energy re-\\nquirements. If suitably harnessed, this highly diffused source has the potential\\nto satisfy all future energy needs. In the 21st century solar energy has become\\nincreasingly attractive as a renewable energy source because of its inexhaustible\\nsupply and its nonpolluting character, in stark contrast to the finite fossil fuels\\ncoal, petroleum, and natural gas. See also solar power.\\n1' metadata={'source': 'About_solar2.pdf', 'file_path': 'About_solar2.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX via pandoc', 'producer': 'xdvipdfmx (20211117)', 'creationDate': \"D:20250314112713-00'00'\", 'modDate': '', 'trapped': ''}\n",
      "page_content='There are two type Active solar and passive solar. Active solar techniques include the use of photovoltaic systems, concentrated solar power, \\nand solar water heating to harness the energy. Passive solar techniques include designing a building for better daylighting, selecting materials \\nwith favorable thermal mass or light-dispersing properties, and organizing spaces that naturally circulate air.' metadata={'source': 'type_solar.pdf', 'file_path': 'type_solar.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025842Z', 'modDate': '', 'trapped': ''}\n",
      "page_content='In 2011, the International Energy Agency said that \"the development of affordable, inexhaustible and clean solar energy technologies will have \\nhuge longer-term benefits. It will increase countries\\' energy security through reliance on an indigenous, inexhaustible, and mostly import-\\nindependent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming. these advantages are global\".' metadata={'source': 'international_conferene.pdf', 'file_path': 'international_conferene.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025902Z', 'modDate': '', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "# generated_prompt.format(\n",
    "#     context = \"Majorly i was inovolved with finance industry, as data analayst\",\n",
    "#     question = \"What role and industry were you inolved with?\"\n",
    "# \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "def process_multiple_pdfs(pdf_files):\n",
    "    # Initialize the RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=700,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    # Create a list to store all the chunks\n",
    "    all_chunks = []\n",
    "\n",
    "    # Iterate over each PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        # Load the document using PyMuPDFLoader\n",
    "        loader = PyMuPDFLoader(pdf_file)\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Process each document and split into chunks\n",
    "        for document in documents:\n",
    "            chunks = text_splitter.split_documents([document])\n",
    "            # Append all chunks to the list\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "    # Return the chunks\n",
    "    return all_chunks\n",
    "\n",
    "# List of PDFs to process\n",
    "pdf_files = [\"About_solar.pdf\", \"About_solar2.pdf\",\"type_solar.pdf\", \"international_conferene.pdf\"]\n",
    "\n",
    "# Process and return all chunks\n",
    "documents = process_multiple_pdfs(pdf_files)\n",
    "\n",
    "# Print results\n",
    "print(f\"Total number of chunks: {len(documents)}\")\n",
    "for chunk in documents:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fedde22-a0b8-4e02-9a0b-9a7ce3efccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible questions to ask\n",
    "#what is solar energy\n",
    "#what are the active solar techniques\n",
    "#what are passive solar techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27379583-aec6-4510-a402-d5fc6a62a402",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "* Document loaders : Load documents from many different sources (HTML, PDF, code).\n",
    "* Document transformers : One of the essential steps in document retrieval is breaking down a large document into smaller, relevant chunks to enhance the retrieval process.\n",
    "* Text embedding models : Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar.\n",
    "* Vector stores: there has emerged a need for databases to support efficient storage and searching of these embeddings.\n",
    "* Retrievers : Once the data is in the database, you still need to retrieve it.\n",
    "## Document Loaders\n",
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be272ee-e810-477c-8c90-011252991d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_docs = 'About_solar.pdf'\n",
    "\n",
    "# loader = PyMuPDFLoader(nlp_docs)\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3b9052-af3b-4eb4-ab45-c676b1c7bf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d217590-e8a4-4718-9592-5ca899cdb895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='solar energy, radiation from the Sun capable of producing heat, causing chemical\\nreactions, or generating electricity. The total amount of solar energy incident\\non Earth is vastly in excess of the world’s current and anticipated energy re-\\nquirements. If suitably harnessed, this highly diffused source has the potential\\nto satisfy all future energy needs. In the 21st century solar energy has become\\nincreasingly attractive as a renewable energy source because of its inexhaustible\\nsupply and its nonpolluting character, in stark contrast to the finite fossil fuels\\ncoal, petroleum, and natural gas. See also solar power.\\n1', metadata={'source': 'About_solar2.pdf', 'file_path': 'About_solar2.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX via pandoc', 'producer': 'xdvipdfmx (20211117)', 'creationDate': \"D:20250314112713-00'00'\", 'modDate': '', 'trapped': ''})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af69e01-6ba6-4a18-884d-e28571c13983",
   "metadata": {},
   "source": [
    "## Document Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b3c95c-de8f-46bf-bdf7-1b8eff9274cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Solar energy is the radiant energy from the Sun's light and heat, which can be harnessed using a range of technologies such as solar electricity, \\nsolar thermal energy (including solar water heating) and solar architecture. It is an essential source of renewable energy, and its technologies \\nare broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into \\nsolar power.\" metadata={'source': 'About_solar.pdf', 'file_path': 'About_solar.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025650Z', 'modDate': '', 'trapped': ''}\n",
      "--------------------\n",
      "page_content='solar energy, radiation from the Sun capable of producing heat, causing chemical\\nreactions, or generating electricity. The total amount of solar energy incident\\non Earth is vastly in excess of the world’s current and anticipated energy re-\\nquirements. If suitably harnessed, this highly diffused source has the potential\\nto satisfy all future energy needs. In the 21st century solar energy has become\\nincreasingly attractive as a renewable energy source because of its inexhaustible\\nsupply and its nonpolluting character, in stark contrast to the finite fossil fuels\\ncoal, petroleum, and natural gas. See also solar power.\\n1' metadata={'source': 'About_solar2.pdf', 'file_path': 'About_solar2.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX via pandoc', 'producer': 'xdvipdfmx (20211117)', 'creationDate': \"D:20250314112713-00'00'\", 'modDate': '', 'trapped': ''}\n",
      "--------------------\n",
      "page_content='There are two type Active solar and passive solar. Active solar techniques include the use of photovoltaic systems, concentrated solar power, \\nand solar water heating to harness the energy. Passive solar techniques include designing a building for better daylighting, selecting materials \\nwith favorable thermal mass or light-dispersing properties, and organizing spaces that naturally circulate air.' metadata={'source': 'type_solar.pdf', 'file_path': 'type_solar.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025842Z', 'modDate': '', 'trapped': ''}\n",
      "--------------------\n",
      "page_content='In 2011, the International Energy Agency said that \"the development of affordable, inexhaustible and clean solar energy technologies will have \\nhuge longer-term benefits. It will increase countries\\' energy security through reliance on an indigenous, inexhaustible, and mostly import-\\nindependent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming. these advantages are global\".' metadata={'source': 'international_conferene.pdf', 'file_path': 'international_conferene.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025902Z', 'modDate': '', 'trapped': ''}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "doc = text_splitter.split_documents(documents)\n",
    "\n",
    "for chunk in doc:\n",
    "    print(chunk)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b97d7df8-65c3-425d-80c7-632309590d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159dec08-33db-4f82-ba72-528815d57069",
   "metadata": {},
   "source": [
    "## Text Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e8adc-cdc7-4000-8608-dbaaa8209ea6",
   "metadata": {},
   "source": [
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n",
    "\n",
    "Note Instructor Model : Huggingface | Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c683944-498d-4bcf-8c37-ed1c708c3804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# Set the device (e.g., 'cuda' or 'cpu')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": device}  # Avoid passing any unsupported arguments like 'token'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c5d664-bc92-4b7d-84ef-388e948d43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.1.2\n",
    "# !pip install -U sentence-transformers==2.2.2\n",
    "# !pip install huggingface-hub==0.25.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c683fdd-2203-450e-8eb0-1025f324cb27",
   "metadata": {},
   "source": [
    "## Vector-store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10f914ce-c707-46ff-b528-748aaf860ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "vector_path = './vector-store'\n",
    "if not os.path.exists(vector_path):\n",
    "    os.makedirs(vector_path)\n",
    "    print('create path done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4eb7a48-e399-496b-b6a9-9faec86c95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path where index is saved: /home/jupyter-st125490/vector_db_path/nlp_stanford\n",
      "FAISS index saved to: /home/jupyter-st125490/vector_db_path/nlp_stanford\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "# Assuming 'documents' is your list of Document objects\n",
    "texts = [doc.page_content for doc in documents]\n",
    "metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "# Create a list of tuples (text, embedding)\n",
    "text_embedding_pairs = zip(texts, embeddings)\n",
    "\n",
    "# Initialize FAISS Vector Store from embeddings\n",
    "vector_path = './vector_db_path'  # Replace with your desired path\n",
    "db_file_name = 'nlp_stanford'\n",
    "vector_path = os.path.abspath(vector_path)\n",
    "full_path = os.path.join(vector_path, db_file_name)\n",
    "print(f\"Full path where index is saved: {full_path}\")\n",
    "\n",
    "vectordb = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embedding_pairs,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas  # Include metadata separately\n",
    ")\n",
    "\n",
    "# Save the FAISS index to a specified local path\n",
    "vectordb.save_local(\n",
    "    folder_path=full_path,\n",
    "    index_name='nlp'  # Default index name\n",
    ")\n",
    "\n",
    "print(f\"FAISS index saved to: {os.path.join(vector_path, db_file_name)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075d15c-7c0e-4d45-8601-519196b161d0",
   "metadata": {},
   "source": [
    "## Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "166e4fe1-be05-4676-aff1-801af6208b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path = full_path,\n",
    "    embeddings = embedding_model,\n",
    "    index_name = 'nlp' #default index\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ae387d4-b7f5-426b-9f01-76f0f0859789",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is solar energy?\"\n",
    "\n",
    "# Embed the query using the same embedding model\n",
    "query_embedding = embedding_model.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb98bb5c-d134-458b-a985-7587fa13f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Solar energy is the radiant energy from the Sun's light and heat, which can be harnessed using a range of technologies such as solar electricity, \\nsolar thermal energy (including solar water heating) and solar architecture. It is an essential source of renewable energy, and its technologies \\nare broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into \\nsolar power.\" metadata={'source': 'About_solar.pdf', 'file_path': 'About_solar.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025650Z', 'modDate': '', 'trapped': ''}\n",
      "page_content='solar energy, radiation from the Sun capable of producing heat, causing chemical\\nreactions, or generating electricity. The total amount of solar energy incident\\non Earth is vastly in excess of the world’s current and anticipated energy re-\\nquirements. If suitably harnessed, this highly diffused source has the potential\\nto satisfy all future energy needs. In the 21st century solar energy has become\\nincreasingly attractive as a renewable energy source because of its inexhaustible\\nsupply and its nonpolluting character, in stark contrast to the finite fossil fuels\\ncoal, petroleum, and natural gas. See also solar power.\\n1' metadata={'source': 'About_solar2.pdf', 'file_path': 'About_solar2.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX via pandoc', 'producer': 'xdvipdfmx (20211117)', 'creationDate': \"D:20250314112713-00'00'\", 'modDate': '', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query, k=2)  # 'k' is the number of similar documents you want\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c82b7872-1475-49ec-9bdf-6c06a9ab2e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was the international conference done?\"\n",
    "\n",
    "# Embed the query using the same embedding model\n",
    "query_embedding = embedding_model.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad340c3e-c9ca-4671-b6ea-1d554972d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='In 2011, the International Energy Agency said that \"the development of affordable, inexhaustible and clean solar energy technologies will have \\nhuge longer-term benefits. It will increase countries\\' energy security through reliance on an indigenous, inexhaustible, and mostly import-\\nindependent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming. these advantages are global\".' metadata={'source': 'international_conferene.pdf', 'file_path': 'international_conferene.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025902Z', 'modDate': '', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query, k=1)  # 'k' is the number of similar documents you want\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4faf3d8-90d8-4891-bc42-f63f76145d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='There are two type Active solar and passive solar. Active solar techniques include the use of photovoltaic systems, concentrated solar power, \\nand solar water heating to harness the energy. Passive solar techniques include designing a building for better daylighting, selecting materials \\nwith favorable thermal mass or light-dispersing properties, and organizing spaces that naturally circulate air.', metadata={'source': 'type_solar.pdf', 'file_path': 'type_solar.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025842Z', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content=\"Solar energy is the radiant energy from the Sun's light and heat, which can be harnessed using a range of technologies such as solar electricity, \\nsolar thermal energy (including solar water heating) and solar architecture. It is an essential source of renewable energy, and its technologies \\nare broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into \\nsolar power.\", metadata={'source': 'About_solar.pdf', 'file_path': 'About_solar.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025650Z', 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='solar energy, radiation from the Sun capable of producing heat, causing chemical\\nreactions, or generating electricity. The total amount of solar energy incident\\non Earth is vastly in excess of the world’s current and anticipated energy re-\\nquirements. If suitably harnessed, this highly diffused source has the potential\\nto satisfy all future energy needs. In the 21st century solar energy has become\\nincreasingly attractive as a renewable energy source because of its inexhaustible\\nsupply and its nonpolluting character, in stark contrast to the finite fossil fuels\\ncoal, petroleum, and natural gas. See also solar power.\\n1', metadata={'source': 'About_solar2.pdf', 'file_path': 'About_solar2.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX via pandoc', 'producer': 'xdvipdfmx (20211117)', 'creationDate': \"D:20250314112713-00'00'\", 'modDate': '', 'trapped': ''}),\n",
       " Document(page_content='In 2011, the International Energy Agency said that \"the development of affordable, inexhaustible and clean solar energy technologies will have \\nhuge longer-term benefits. It will increase countries\\' energy security through reliance on an indigenous, inexhaustible, and mostly import-\\nindependent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming. these advantages are global\".', metadata={'source': 'international_conferene.pdf', 'file_path': 'international_conferene.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.3', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'pdfmake', 'producer': 'pdfmake', 'creationDate': 'D:20250314025902Z', 'modDate': '', 'trapped': ''})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path = full_path,\n",
    "    embeddings = embedding_model,\n",
    "    index_name = 'nlp' #default index\n",
    ")   \n",
    "#ready to use\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever.get_relevant_documents(\"what are two types of solar techniques?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2aa449-788d-4826-8992-bbe9925b457f",
   "metadata": {},
   "source": [
    "**The above retrival without similarity index based returns the top one as the relevant document**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595bd57-567d-4e75-ba49-c726b1eb5e8f",
   "metadata": {},
   "source": [
    "## Task 1.3 Explore the use of other text-generation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245b2028-ea34-433b-9e31-9f7a383cc318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 0.1.2\n",
      "Uninstalling langchain-0.1.2:\n",
      "  Successfully uninstalled langchain-0.1.2\n",
      "Found existing installation: langchain-core 0.3.45\n",
      "Uninstalling langchain-core-0.3.45:\n",
      "  Successfully uninstalled langchain-core-0.3.45\n",
      "\u001b[33mWARNING: Skipping langchain-groq as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y langchain langchain-core langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdd6a14-2d22-4fe2-9df0-aafc695eb936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b7d8432-eb8b-4e72-9b6b-3cf69bc138fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index\n",
    "# !pip install llama-index-llms-groq\n",
    "# !pip install llama-index-embeddings-huggingface\n",
    "# !pip install llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511df86e-194c-4500-a48e-43942cdc38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "#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\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9d817d-9fcf-43f2-af02-bf68c4be1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "llm = Groq(model=\"llama3-8b-8192\")\n",
    "llm_70b = Groq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "598be8d0-f45d-430f-8ee3-3c44af077d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:11:29.528952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741961489.551483 4152585 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741961489.558501 4152585 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 14:11:29.584060: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df4348c5-00bd-4617-b764-38db4d51a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-index-vector-stores-chroma in ./.local/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: chromadb>=0.5.17 in ./.local/lib/python3.12/site-packages (from llama-index-vector-stores-chroma) (0.6.3)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in ./.local/lib/python3.12/site-packages (from llama-index-vector-stores-chroma) (0.12.24)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/tljh/user/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.10.4)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.115.11)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.local/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.26.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.20.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/tljh/user/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/tljh/user/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/tljh/user/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/tljh/user/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/tljh/user/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.local/lib/python3.12/site-packages (from chromadb>=0.5.17->llama-index-vector-stores-chroma) (13.9.4)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/tljh/user/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2024.12.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/tljh/user/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (11.1.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/tljh/user/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.local/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.18.3)\n",
      "Requirement already satisfied: packaging>=19.1 in ./.local/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (23.2)\n",
      "Requirement already satisfied: pyproject_hooks in ./.local/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in ./.local/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.46.1)\n",
      "Requirement already satisfied: anyio in ./.local/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.7.1)\n",
      "Requirement already satisfied: certifi in /opt/tljh/user/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/tljh/user/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/tljh/user/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/tljh/user/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/tljh/user/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/tljh/user/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/tljh/user/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/tljh/user/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/tljh/user/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./.local/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.9)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (2024.11.6)\n",
      "Requirement already satisfied: coloredlogs in ./.local/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.local/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (25.1.24)\n",
      "Requirement already satisfied: protobuf in ./.local/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.29.3)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in ./.local/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.local/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.69.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.0 in ./.local/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.0 in ./.local/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.31.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b0 in ./.local/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b0 in ./.local/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b0 in ./.local/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b0 in ./.local/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.52b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./.local/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.52b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.local/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.local/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/tljh/user/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic>=1.9->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.local/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/tljh/user/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.1.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.local/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.29.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.local/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.local/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./.local/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.local/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.local/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.local/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.local/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.17->llama-index-vector-stores-chroma) (15.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.local/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-chroma) (3.26.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.local/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.local/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (4.9)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.16.1)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.local/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/tljh/user/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.local/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.17->llama-index-vector-stores-chroma) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./.local/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.17->llama-index-vector-stores-chroma) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52796d59-e4ac-4711-a11b-55f1eef43a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "# Assuming 'documents' is your list of Document objects\n",
    "texts = [doc.page_content for doc in documents]\n",
    "metadatas = [doc.metadata for doc in documents]\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\",  model_kwargs={\"trust_remote_code\": True})\n",
    "# Initialize Chroma client\n",
    "client = 0\n",
    "client = chromadb.Client()\n",
    "\n",
    "#Set your collection name (make sure it's a string)\n",
    "for i, doc in enumerate(documents):\n",
    "    collection_name = f\"collection_{i+20}\"\n",
    "    collection = client.create_collection(collection_name)\n",
    "    collection.add(\n",
    "        documents=doc.page_content,\n",
    "        ids=[str(i)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1b6fbe6-90b3-43c5-95ad-57547bba2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class CustomChromaManager:\n",
    "    def __init__(self, model_name: str, model_kwargs: dict = None, encode_kwargs: dict = None):\n",
    "        self.model_name = model_name\n",
    "        self.model_kwargs = model_kwargs or {}\n",
    "        self.encode_kwargs = encode_kwargs or {}\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, **self.model_kwargs)\n",
    "        self.model = AutoModel.from_pretrained(model_name, **self.model_kwargs)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Initialize Chroma client\n",
    "        self.client = chromadb.Client()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu().numpy().tolist()\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.embed_documents([query])[0]\n",
    "\n",
    "    def create_and_add_documents(self, documents):\n",
    "        # Group documents by collection name\n",
    "        collections = {}\n",
    "        for i, doc in enumerate(documents):\n",
    "            collection_name = f\"collection_{i+20}\"\n",
    "            if collection_name not in collections:\n",
    "                collections[collection_name] = {'texts': [], 'ids': []}\n",
    "            collections[collection_name]['texts'].append(doc.page_content)\n",
    "            collections[collection_name]['ids'].append(str(i))\n",
    "\n",
    "        # Create collections and add documents\n",
    "        for collection_name, data in collections.items():\n",
    "            collection = self.client.create_collection(collection_name)\n",
    "            embeddings = self.embed_documents(data['texts'])\n",
    "            collection.add(\n",
    "                documents=data['texts'],\n",
    "                ids=data['ids'],\n",
    "                embeddings=embeddings\n",
    "            )\n",
    "\n",
    "    def similarity_search(self, query: str, collection_name: str, k: int = 5):\n",
    "        collection = self.client.get_collection(collection_name)\n",
    "        query_embedding = self.embed_query(query)\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=k\n",
    "        )\n",
    "        return results['documents']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe4edf9c-f80a-4604-a0aa-e260212b43b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Solar energy is the radiant energy from the Sun's light and heat, which can be harnessed using a range of technologies such as solar electricity, \\nsolar thermal energy (including solar water heating) and solar architecture. It is an essential source of renewable energy, and its technologies \\nare broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into \\nsolar power.\"]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the custom Chroma manager\n",
    "chroma_manager = CustomChromaManager(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={\"trust_remote_code\": True},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "# Assuming 'documents' is your list of Document objects\n",
    "chroma_manager.create_and_add_documents(documents)\n",
    "\n",
    "# Perform a similarity search on a specific collection\n",
    "query = \"What is solar energy?\"\n",
    "collection_name = \"collection_20\"  # Replace with your collection name\n",
    "results = chroma_manager.similarity_search(query, collection_name)\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024680ab-6c5c-4bae-85ac-236df8a64bad",
   "metadata": {},
   "source": [
    "## Task 2.1 Provide a list of the retriever and generator models you have utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f346188-cbbe-4015-81ab-ad82bed54687",
   "metadata": {},
   "source": [
    "In Our setup, you've integrated multiple retrievers and generators to enhance your application's capabilities. Here's a breakdown of each component:\n",
    "\n",
    "Retrievers:\n",
    "* Instruction-Based Similarity Distance Retriever:\n",
    "        This retriever utilizes a similarity metric, such as cosine similarity or Euclidean distance, to find documents that closely match the provided instruction or query. It's particularly effective when you have a well-defined set of documents and need to retrieve those most relevant to a specific instruction.\n",
    "* Indexed Retriever:\n",
    "        An indexed retriever relies on a pre-built index of documents, enabling efficient retrieval based on exact matches or keyword searches. This method is suitable when your dataset is relatively static, and you require fast retrieval based on specific terms or phrases.\n",
    "* Groq Retriever:\n",
    "        The Groq retriever leverages Groq's AI inference technology to perform rapid and efficient document retrieval. By utilizing Groq's Language Processing Unit (LPU), it accelerates the retrieval process, making it suitable for applications requiring high-performance inference.\n",
    "\n",
    "Generators:\n",
    "* Instruction-Based Generator:\n",
    "        This generator is designed to produce responses or content based on specific instructions. By interpreting the given instruction, it generates relevant and contextually appropriate text, making it useful for tasks like content creation, summarization, or Q&A systems.\n",
    "* Groq API Generator:\n",
    "        The Groq API generator utilizes Groq's advanced AI processing capabilities to generate content. By interfacing with Groq's API, it can produce high-quality text responses, benefiting applications that require scalable and efficient text generation.\n",
    "\n",
    "By combining these retrievers and generators, the application can effectively handle a wide range of tasks, from precise document retrieval to dynamic content generation, all while maintaining high performance and relevance to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35705aa7-a46d-484c-9c9d-7112bfa94242",
   "metadata": {},
   "source": [
    "## Task 2.2 Analyze any issues related to the models providing unrelated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0ae4b68-2925-49f1-ab22-b8f16aeee1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain==0.1.2\n",
      "  Using cached langchain-0.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.2) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.2) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.local/lib/python3.12/site-packages (from langchain==0.1.2) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.local/lib/python3.12/site-packages (from langchain==0.1.2) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.2) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in ./.local/lib/python3.12/site-packages (from langchain==0.1.2) (0.0.18)\n",
      "Collecting langchain-core<0.2,>=0.1.14 (from langchain==0.1.2)\n",
      "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.0.84,>=0.0.83 (from langchain==0.1.2)\n",
      "  Using cached langsmith-0.0.83-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.local/lib/python3.12/site-packages (from langchain==0.1.2) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.2) (2.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from langchain==0.1.2) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.local/lib/python3.12/site-packages (from langchain==0.1.2) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.2) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.2) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.local/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.2) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/tljh/user/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.2) (3.0.0)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.2,>=0.1.14 (from langchain==0.1.2)\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./.local/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.14->langchain==0.1.2) (3.7.1)\n",
      "  Using cached langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.22-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.21-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.20-py3-none-any.whl.metadata (6.0 kB)\n",
      "  Using cached langchain_core-0.1.19-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./.local/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.14->langchain==0.1.2) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.1.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.1.2) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/tljh/user/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.1.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.1.2) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.2) (3.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/tljh/user/lib/python3.12/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.14->langchain==0.1.2) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.local/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.2) (1.0.0)\n",
      "Using cached langchain-0.1.2-py3-none-any.whl (803 kB)\n",
      "Using cached langchain_core-0.1.19-py3-none-any.whl (238 kB)\n",
      "Using cached langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
      "Installing collected packages: langsmith, langchain-core, langchain\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.3.13\n",
      "    Uninstalling langsmith-0.3.13:\n",
      "      Successfully uninstalled langsmith-0.3.13\n",
      "\u001b[33m  WARNING: The script langsmith is installed in '/home/jupyter-st125490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.45\n",
      "    Uninstalling langchain-core-0.3.45:\n",
      "      Successfully uninstalled langchain-core-0.3.45\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.20\n",
      "    Uninstalling langchain-0.3.20:\n",
      "      Successfully uninstalled langchain-0.3.20\n",
      "\u001b[33m  WARNING: The script langchain-server is installed in '/home/jupyter-st125490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-text-splitters 0.3.6 requires langchain-core<1.0.0,>=0.3.34, but you have langchain-core 0.1.19 which is incompatible.\n",
      "langchain-groq 0.2.5 requires langchain-core<1.0.0,>=0.3.42, but you have langchain-core 0.1.19 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-0.1.2 langchain-core-0.1.19 langsmith-0.0.83\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f2c2a-9cbe-4af5-8736-82b932761892",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eaab6f53-f6e5-487e-8c5e-85bb66fb53ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e867795-ba6e-48fe-8286-f59bb5ff693f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='hi'), AIMessage(content='Whats up?'), HumanMessage(content='How are you'), AIMessage(content=\"I'm quite good. How about you?\")])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_user_message('hi')\n",
    "history.add_ai_message('Whats up?')\n",
    "history.add_user_message('How are you')\n",
    "history.add_ai_message('I\\'m quite good. How about you?')\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a6847-4dc3-4d7c-bad4-c5689cfd4f69",
   "metadata": {},
   "source": [
    "**Conversation Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a02ed160-db05-4d3d-ba26-4d655bff9cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi\\nAI: What's up?\\nHuman: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d049a757-5541-4e7d-994d-411b74228631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'),\n",
       "  AIMessage(content=\"What's up?\"),\n",
       "  HumanMessage(content='How are you?'),\n",
       "  AIMessage(content=\"I'm quite good. How about you?\")]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages = True)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4eb94-afa6-4054-b7dd-7e58e3342f98",
   "metadata": {},
   "source": [
    "**Conversation Buffer Window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f76019bc-9bae-4123-8eef-13bf8b513c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: How are you?\\nAI: I'm quite good. How about you?\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({'input':'hi'}, {'output':'What\\'s up?'})\n",
    "memory.save_context({\"input\":'How are you?'},{'output': 'I\\'m quite good. How about you?'})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e82e5-2da5-494a-827a-d99a242f09d3",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfc0f8c3-4f92-4167-b7d7-633e885daf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:24:10.393285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741991050.417474    2835 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741991050.424966    2835 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 22:24:10.453141: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/jupyter-st125490/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "# from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "model_id = 'lmsys/fastchat-t5-3b-v1.0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c452e2b-d0ba-45d7-8964-8a3cbee3ef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import BitsAndBytesConfig\n",
    "# Load tokenizer and model\n",
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set up BitsAndBytes for 4-bit quantization\n",
    "# bitsandbyte_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ec09f1-5477-4082-940c-57c64aaa8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# bitsandbyte_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit = True,\n",
    "#     bnb_4bit_quant_type = \"nf4\",\n",
    "#     load_in_8bit_fp32_cpu_offload=True,\n",
    "#     bnb_4bit_compute_dtype = torch.float16,\n",
    "#     bnb_4bit_use_double_quant = True\n",
    "# )\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = 'auto'\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens = 256,\n",
    "    model_kwargs = {\n",
    "        \"temperature\" : 0,\n",
    "        \"repetition_penalty\": 1.5\n",
    "    }\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c22907e4-5029-40cf-8c5e-6d48dffa38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let us load all the 10 questions to answer\n",
    "# 1) How old are you?\n",
    "# 2) What is your highest level of education?\n",
    "# 3) What major or field of study did you pursue during your education?\n",
    "# 4) How many years of work experience do you have?\n",
    "# 5) What type of work or industry have you been involved in?\n",
    "# 6) Can you describe your current role or job responsibilities?\n",
    "# 7) What are your core beliefs regarding the role of technology in shaping society?\n",
    "# 8) How do you think cultural values should influence technological advancements ?\n",
    "# 9) As a master’s student, what is the most challenging aspect of your studies so far?\n",
    "# 10) What specific research interests or academic goals do you hope to achieve during your time as a\n",
    "# master’s student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd92f010-c36c-40ef-b0d0-b8b19bdb30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"I supppose you are 29 year old\", \n",
    "         \"The hiher education i have is graduation\",\n",
    "         \"The decipline of study is computer science\",\n",
    "         \"approximatly 4 years of working expereince, do you want to know more about\",\n",
    "         \"The working industry or area was IT and Finance majorly expereince as data anlyst and Data engineer at fintech startups\",\n",
    "         \"Currently as if i am pursing masters so industrial role is out of the question\",\n",
    "         \"Technology can shape the soceity with their daily needs, you see the paper pin to the big aeroplans all humans are these days relays on technology and to the matter of fact in order to create the better world around us, any other question you have\", \n",
    "        \"Culture and technology can go hand in hand, with that said culture is a traditional to the human value or create a ethical behaviour (to say what is wrong or right), where as technology is need of thd hour can have its own pros and cons\",\n",
    "         \"As a masters student the most difficult part is coming back to the idea of acadamic thinking while if one have world in industry for a while, could be a little challanging\", \n",
    "         \"The majory research interest area i want to work is the non-subjective matters of solving AI problems, that is niche to the field rather than going with application that everybody is doing\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c51901dc-c818-45a0-8ae3-fcfdda464246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='I supppose you are 29 year old'\n",
      "--------------------\n",
      "page_content='The hiher education i have is graduation'\n",
      "--------------------\n",
      "page_content='The decipline of study is computer science'\n",
      "--------------------\n",
      "page_content='approximatly 4 years of working expereince, do you want to know more about'\n",
      "--------------------\n",
      "page_content='The working industry or area was IT and Finance majorly expereince as data anlyst and Data engineer at fintech startups'\n",
      "--------------------\n",
      "page_content='Currently as if i am pursing masters so industrial role is out of the question'\n",
      "--------------------\n",
      "page_content='Technology can shape the soceity with their daily needs, you see the paper pin to the big aeroplans all humans are these days relays on technology and to the matter of fact in order to create the better world around us, any other question you have'\n",
      "--------------------\n",
      "page_content='Culture and technology can go hand in hand, with that said culture is a traditional to the human value or create a ethical behaviour (to say what is wrong or right), where as technology is need of thd hour can have its own pros and cons'\n",
      "--------------------\n",
      "page_content='As a masters student the most difficult part is coming back to the idea of acadamic thinking while if one have world in industry for a while, could be a little challanging'\n",
      "--------------------\n",
      "page_content='The majory research interest area i want to work is the non-subjective matters of solving AI problems, that is niche to the field rather than going with application that everybody is doing'\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define a template to answer questions about solar energy based on a single article\n",
    "personal_prompt_template = PromptTemplate(\n",
    "    input_variables=[],  # No dynamic input variables in this case\n",
    "    template=\"\"\"\n",
    "    You are an assistant to provide information about myself, please ask questions:\n",
    "    1) How old are you?\n",
    "    2) What is your highest level of education?\n",
    "    3) What major or field of study did you pursue during your education?\n",
    "    4) How many years of work experience do you have?\n",
    "    5) What type of work or industry have you been involved in?\n",
    "    6) Can you describe your current role or job responsibilities?\n",
    "    7) What are your core beliefs regarding the role of technology in shaping society?\n",
    "    8) How do you think cultural values should influence technological advancements?\n",
    "    9) As a master’s student, what is the most challenging aspect of your studies so far?\n",
    "    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# PROMPT = PromptTemplate.from_template(\n",
    "#     template = personal_prompt_template\n",
    "# )\n",
    "\n",
    "documents = []\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "# Process each document and split into chunks\n",
    "docs = text_splitter.create_documents(texts)\n",
    "for document in docs:\n",
    "    chunks = text_splitter.split_text(document.page_content)\n",
    "    # print(chunks)\n",
    "    # doc = text_splitter.split_documents(chunks)\n",
    "    # Append all chunks to the list\n",
    "    documents.extend(chunks)\n",
    "\n",
    "doc = text_splitter.split_documents(docs)\n",
    "\n",
    "for chunk in doc:\n",
    "    print(chunk)\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "57a1d4b5-0957-4467-8189-83bc0d604c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    You are an assistant to provide information about myself, please ask questions:\\n    1) How old are you?\\n    2) What is your highest level of education?\\n    3) What major or field of study did you pursue during your education?\\n    4) How many years of work experience do you have?\\n    5) What type of work or industry have you been involved in?\\n    6) Can you describe your current role or job responsibilities?\\n    7) What are your core beliefs regarding the role of technology in shaping society?\\n    8) How do you think cultural values should influence technological advancements?\\n    9) As a master’s student, what is the most challenging aspect of your studies so far?\\n    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\\n\\n    Context: The working industry or area was IT and Finance majorly expereince as data anlyst and Data engineer at fintech startups\\n    \""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(\n",
    "    context = \"The working industry or area was IT and Finance majorly expereince as data anlyst and Data engineer at fintech startups\" ,\n",
    "    question = \"Which company i was involves with\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d148fc5-71b3-4260-97d9-b25816cf3a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803cbf4f-f70c-49cd-a019-82105885d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2348523-ad93-4bb1-b996-60d96f990bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = CONDENSE_QUESTION_PROMPT,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba596c33-9736-4406-8c18-ed0be7b58b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "Human:What is my age?\n",
      "AI:\n",
      "Human:Which industry i was involved with?\n",
      "AI:\n",
      "Follow Up Input: Comparing both of them\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Human:What is my age?\\nAI:\\nHuman:Which industry i was involved with?\\nAI:',\n",
       " 'question': 'Comparing both of them',\n",
       " 'text': '<pad> What  is  your  age?\\n'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Comparing both of them'\n",
    "chat_history = \"Human:What is my age?\\nAI:\\nHuman:Which industry i was involved with?\\nAI:\"\n",
    "\n",
    "question_generator({'chat_history' : chat_history, \"question\" : query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b2f60358-9752-44fa-9617-ef8720c91a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path where index is saved: /home/jupyter-st125490/vector_db_path_paersonal/nlp_stanford\n",
      "FAISS index saved to: /home/jupyter-st125490/vector_db_path_paersonal/nlp_stanford\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "texts = [d.page_content for d in doc]\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "# Create a list of tuples (text, embedding)\n",
    "text_embedding_pairs = zip(texts, embeddings)\n",
    "\n",
    "# Initialize FAISS Vector Store from embeddings\n",
    "vector_path = './vector_db_path_paersonal'  # Replace with your desired path\n",
    "db_file_name = 'nlp_stanford'\n",
    "vector_path = os.path.abspath(vector_path)\n",
    "full_path = os.path.join(vector_path, db_file_name)\n",
    "print(f\"Full path where index is saved: {full_path}\")\n",
    "\n",
    "vectordb = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embedding_pairs,\n",
    "    embedding=embedding_model)\n",
    "\n",
    "# Save the FAISS index to a specified local path\n",
    "vectordb.save_local(\n",
    "    folder_path=full_path,\n",
    "    index_name='nlp'  # Default index name\n",
    ")\n",
    "\n",
    "print(f\"FAISS index saved to: {os.path.join(vector_path, db_file_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c20e1dbb-7e07-4ffe-bf20-6c1e5605acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.load_local(\n",
    "    folder_path = full_path,\n",
    "    embeddings = embedding_model,\n",
    "    index_name = 'nlp' #default index\n",
    ")   \n",
    "#ready to use\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a1446e26-a412-4364-928e-37f3ee873c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define memory for conversation history (stores last 3 interactions)\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",  # Key for the conversation history\n",
    "    output_key=\"response\",      # Key for the model's output\n",
    "    k=3,  # Keep last 3 interactions\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Define the prompt template with input variables\n",
    "prompt_temp = \"\"\"\n",
    "    You are an assistant to provide information about myself, please ask questions:\n",
    "    1) How old are you?\n",
    "    2) What is your highest level of education?\n",
    "    3) What major or field of study did you pursue during your education?\n",
    "    4) How many years of work experience do you have?\n",
    "    5) What type of work or industry have you been involved in?\n",
    "    6) Can you describe your current role or job responsibilities?\n",
    "    7) What are your core beliefs regarding the role of technology in shaping society?\n",
    "    8) How do you think cultural values should influence technological advancements?\n",
    "    9) As a master’s student, what is the most challenging aspect of your studies so far?\n",
    "    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\n",
    "\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template = prompt_temp)\n",
    "\n",
    "\n",
    "# Create LLMChain with memory and the prompt\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example query with context\n",
    "query = \"What type of work or industry have you been involved in?\"\n",
    "context = \"The working industry or area was IT and Finance majorly expereince as data anlyst and Data engineer at fintech startups\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dff00356-7c0f-47d2-af38-39be27cf4119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant to provide information about myself, please ask questions:\\n    1) How old are you?\\n    2) What is your highest level of education?\\n    3) What major or field of study did you pursue during your education?\\n    4) How many years of work experience do you have?\\n    5) What type of work or industry have you been involved in?\\n    6) Can you describe your current role or job responsibilities?\\n    7) What are your core beliefs regarding the role of technology in shaping society?\\n    8) How do you think cultural values should influence technological advancements?\\n    9) As a master’s student, what is the most challenging aspect of your studies so far?\\n    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\\n\\n    {context}\\n    Question: {question}\\n    Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x7a81b40366f0>)), document_variable_name='context')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain = load_qa_chain(\n",
    "    llm = llm,\n",
    "    chain_type = 'stuff',\n",
    "    prompt = prompt,\n",
    "    verbose = True\n",
    ")\n",
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9bf17b59-58f0-4bf9-8335-c6051c4d1c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to provide information about myself, please ask questions:\n",
      "    1) How old are you?\n",
      "    2) What is your highest level of education?\n",
      "    3) What major or field of study did you pursue during your education?\n",
      "    4) How many years of work experience do you have?\n",
      "    5) What type of work or industry have you been involved in?\n",
      "    6) Can you describe your current role or job responsibilities?\n",
      "    7) What are your core beliefs regarding the role of technology in shaping society?\n",
      "    8) How do you think cultural values should influence technological advancements?\n",
      "    9) As a master’s student, what is the most challenging aspect of your studies so far?\n",
      "    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\n",
      "\n",
      "    I supppose you are 29 year old\n",
      "\n",
      "approximatly 4 years of working expereince, do you want to know more about\n",
      "\n",
      "The hiher education i have is graduation\n",
      "\n",
      "Currently as if i am pursing masters so industrial role is out of the question\n",
      "    Question: How old are you?\n",
      "    Answer:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125490/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='I supppose you are 29 year old'),\n",
       "  Document(page_content='approximatly 4 years of working expereince, do you want to know more about'),\n",
       "  Document(page_content='The hiher education i have is graduation'),\n",
       "  Document(page_content='Currently as if i am pursing masters so industrial role is out of the question')],\n",
       " 'question': 'How old are you?',\n",
       " 'output_text': '<pad> Answer:  29\\n'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How old are you?\"\n",
    "input_document = retriever.get_relevant_documents(query)\n",
    "\n",
    "doc_chain({'input_documents':input_document, 'question':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c485e1a6-4f79-4aad-95d3-a1303d33a036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(memory=ConversationBufferWindowMemory(output_key='answer', return_messages=True, memory_key='chat_history', k=3), verbose=True, combine_docs_chain=StuffDocumentsChain(verbose=True, llm_chain=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant to provide information about myself, please ask questions:\\n    1) How old are you?\\n    2) What is your highest level of education?\\n    3) What major or field of study did you pursue during your education?\\n    4) How many years of work experience do you have?\\n    5) What type of work or industry have you been involved in?\\n    6) Can you describe your current role or job responsibilities?\\n    7) What are your core beliefs regarding the role of technology in shaping society?\\n    8) How do you think cultural values should influence technological advancements?\\n    9) As a master’s student, what is the most challenging aspect of your studies so far?\\n    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\\n\\n    {context}\\n    Question: {question}\\n    Answer:\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x7a81b40366f0>)), document_variable_name='context'), question_generator=LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text2text_generation.Text2TextGenerationPipeline object at 0x7a81b40366f0>)), return_source_documents=True, get_chat_history=<function <lambda> at 0x7a81a4340720>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7a81a792d160>))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k=3, \n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True,\n",
    "    output_key = 'answer'\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    return_source_documents=True,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    get_chat_history=lambda h : h\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1fafe-1a53-42ff-854e-46c0a482ec02",
   "metadata": {},
   "source": [
    "## chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "53df84c0-24a9-440d-bbd1-d88e5dcd1c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to provide information about myself, please ask questions:\n",
      "    1) How old are you?\n",
      "    2) What is your highest level of education?\n",
      "    3) What major or field of study did you pursue during your education?\n",
      "    4) How many years of work experience do you have?\n",
      "    5) What type of work or industry have you been involved in?\n",
      "    6) Can you describe your current role or job responsibilities?\n",
      "    7) What are your core beliefs regarding the role of technology in shaping society?\n",
      "    8) How do you think cultural values should influence technological advancements?\n",
      "    9) As a master’s student, what is the most challenging aspect of your studies so far?\n",
      "    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\n",
      "\n",
      "    I supppose you are 29 year old\n",
      "\n",
      "approximatly 4 years of working expereince, do you want to know more about\n",
      "\n",
      "The hiher education i have is graduation\n",
      "\n",
      "The decipline of study is computer science\n",
      "    Question: what is my age\n",
      "    Answer:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125490/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'what is my age',\n",
       " 'chat_history': [],\n",
       " 'answer': '<pad> Your  age  is  29.\\n',\n",
       " 'source_documents': [Document(page_content='I supppose you are 29 year old'),\n",
       "  Document(page_content='approximatly 4 years of working expereince, do you want to know more about'),\n",
       "  Document(page_content='The hiher education i have is graduation'),\n",
       "  Document(page_content='The decipline of study is computer science')]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is my age\"\n",
    "answer = chain({\"question\":query})\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "068cbbd0-019c-47dd-81d2-3c0a13dbd9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='what is my age'), AIMessage(content='<pad> Your  age  is  29.\\n')]\n",
      "Follow Up Input: which comany i was involved with?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125490/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to provide information about myself, please ask questions:\n",
      "    1) How old are you?\n",
      "    2) What is your highest level of education?\n",
      "    3) What major or field of study did you pursue during your education?\n",
      "    4) How many years of work experience do you have?\n",
      "    5) What type of work or industry have you been involved in?\n",
      "    6) Can you describe your current role or job responsibilities?\n",
      "    7) What are your core beliefs regarding the role of technology in shaping society?\n",
      "    8) How do you think cultural values should influence technological advancements?\n",
      "    9) As a master’s student, what is the most challenging aspect of your studies so far?\n",
      "    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\n",
      "\n",
      "    The working industry or area was IT and Finance majorly expereince as data anlyst and Data engineer at fintech startups\n",
      "\n",
      "approximatly 4 years of working expereince, do you want to know more about\n",
      "\n",
      "Currently as if i am pursing masters so industrial role is out of the question\n",
      "\n",
      "The majory research interest area i want to work is the non-subjective matters of solving AI problems, that is niche to the field rather than going with application that everybody is doing\n",
      "    Question: <pad> What  company  were  you  involved  with?\n",
      "\n",
      "    Answer:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125490/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'which comany i was involved with?',\n",
       " 'chat_history': [HumanMessage(content='what is my age'),\n",
       "  AIMessage(content='<pad> Your  age  is  29.\\n')],\n",
       " 'answer': '<pad> The  companies  I  was  involved  with  were  fintech  startups.\\n',\n",
       " 'source_documents': [Document(page_content='The working industry or area was IT and Finance majorly expereince as data anlyst and Data engineer at fintech startups'),\n",
       "  Document(page_content='approximatly 4 years of working expereince, do you want to know more about'),\n",
       "  Document(page_content='Currently as if i am pursing masters so industrial role is out of the question'),\n",
       "  Document(page_content='The majory research interest area i want to work is the non-subjective matters of solving AI problems, that is niche to the field rather than going with application that everybody is doing')]}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"which comany i was involved with?\"\n",
    "answer = chain({\"question\":query})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "adece1d1-301d-4188-960a-127dd5b46dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='what is my age'), AIMessage(content='<pad> Your  age  is  29.\\n'), HumanMessage(content='which comany i was involved with?'), AIMessage(content='<pad> The  companies  I  was  involved  with  were  fintech  startups.\\n')]\n",
      "Follow Up Input:  What specific research interests?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125490/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to provide information about myself, please ask questions:\n",
      "    1) How old are you?\n",
      "    2) What is your highest level of education?\n",
      "    3) What major or field of study did you pursue during your education?\n",
      "    4) How many years of work experience do you have?\n",
      "    5) What type of work or industry have you been involved in?\n",
      "    6) Can you describe your current role or job responsibilities?\n",
      "    7) What are your core beliefs regarding the role of technology in shaping society?\n",
      "    8) How do you think cultural values should influence technological advancements?\n",
      "    9) As a master’s student, what is the most challenging aspect of your studies so far?\n",
      "    10) What specific research interests or academic goals do you hope to achieve during your time as a master's student?\n",
      "\n",
      "    The majory research interest area i want to work is the non-subjective matters of solving AI problems, that is niche to the field rather than going with application that everybody is doing\n",
      "\n",
      "Currently as if i am pursing masters so industrial role is out of the question\n",
      "\n",
      "The decipline of study is computer science\n",
      "\n",
      "approximatly 4 years of working expereince, do you want to know more about\n",
      "    Question: <pad> What  are  your  current  research  interests?\n",
      "\n",
      "    Answer:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125490/.local/lib/python3.12/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': ' What specific research interests?',\n",
       " 'chat_history': [HumanMessage(content='what is my age'),\n",
       "  AIMessage(content='<pad> Your  age  is  29.\\n'),\n",
       "  HumanMessage(content='which comany i was involved with?'),\n",
       "  AIMessage(content='<pad> The  companies  I  was  involved  with  were  fintech  startups.\\n')],\n",
       " 'answer': \"<pad>   pad>  My  current  research  interests  are  in  the  non-subjective  aspects  of  solving  AI  problems,  specifically  in  the  area  of  natural  language  processing  and  machine  learning.  I  am  particularly  interested  in  exploring  the  intersection  of  these  two  fields  and  how  they  can  be  used  to  solve  real-world  problems.  I  am  also  interested  in  exploring  the  potential  of  AI  to  improve  healthcare  and  other  areas  of  human  life.  I  am  also  interested  in  exploring  the  potential  of  AI  to  improve  the  quality  of  life  for  people  around  the  world.  I  am  currently  pursuing  a  master's  degree  in  computer  science  and  have  approximately  4  years  of  working  experience.\\n\",\n",
       " 'source_documents': [Document(page_content='The majory research interest area i want to work is the non-subjective matters of solving AI problems, that is niche to the field rather than going with application that everybody is doing'),\n",
       "  Document(page_content='Currently as if i am pursing masters so industrial role is out of the question'),\n",
       "  Document(page_content='The decipline of study is computer science'),\n",
       "  Document(page_content='approximatly 4 years of working expereince, do you want to know more about')]}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \" What specific research interests?\"\n",
    "answer = chain({\"question\":query})\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb141b3-1e76-4af5-9ecb-1bfa13ce1278",
   "metadata": {},
   "source": [
    "## Task 2.2 Analyze any issues related to the models providing unrelated information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e602b-fbb8-4e16-880a-20bc487683d5",
   "metadata": {},
   "source": [
    "If you're encountering issues where the models are providing **unrelated information**, here are a few areas to consider and strategies to help mitigate those issues:\n",
    "\n",
    "### 1. **Issues with Document Retrieval**:\n",
    "   - **Problem**: The retriever might not be pulling the most relevant documents or content, leading to irrelevant responses.\n",
    "   - Or the possibility of having no releavent documents avilable,\n",
    "   - **Solution**: Removing the irrelavent context and stay in-context will return right answers\n",
    "\n",
    "### 2. **Inadequate Prompt Engineering**:\n",
    "   - **Problem**: The way the model is prompted could be insufficient to guide it to return the correct information.\n",
    "   - **Solution**: Here we need to define the propoer prompt structure which helps the model to return the relevant information\n",
    "     \n",
    "### 3. **Model Over-Generation or Hallucination**:\n",
    "   - **Problem**: The model might be generating overly verbose, non-relevant information, commonly referred to as \"hallucinations.\"\n",
    "   - **Solutions**:\n",
    "     - **Limit the response length**: Explicitly tell the model to give concise answers. For example, you could instruct it to \"Answer in no more than 2 sentences.\"\n",
    "     - **Fine-tune on domain-specific data**: Fine-tuning your model on domain-specific content (e.g., information related to your query type) can help reduce hallucinations and improve accuracy.\n",
    "     - **Apply filtering**: Use an additional filtering step to check if the model's generated output aligns with the expected output (e.g., filter out non-related answers).\n",
    "\n",
    "### 4. **Model's Lack of Domain Knowledge**:\n",
    "   - **Problem**: The model may not have the necessary knowledge about the context you're asking it to analyze, especially if it doesn't have enough training data in the relevant domain.\n",
    "   - **Solutions**:\n",
    "     - **Domain-specific fine-tuning**: Fine-tune the model on data that closely matches your use case (e.g., using custom corpora for a specific field like AI or finance).\n",
    "     - **Utilize domain-specific models**: Use models that have been trained on domain-specific data to increase accuracy (e.g., finance-oriented models for financial questions).\n",
    "\n",
    "### 5. **Ambiguity in the Query**:\n",
    "   - **Problem**: The question might be too vague or ambiguous, causing the model to provide a less targeted answer.\n",
    "   - **Solutions**:\n",
    "     - **Clarify the question**: Refine and specify the question better. For example, \"What is my age based on the context I provided?\" instead of just \"How old are you?\"\n",
    "     - **Use disambiguation**: Ask for more specific outputs by adding more context or conditions in the question to guide the model toward a precise answer.\n",
    "\n",
    "### 6. **Inconsistent Memory or State Management**:\n",
    "   - **Problem**: If your model or retriever is using memory but is not properly managing the state of the conversation or context, this could lead to irrelevant outputs.\n",
    "   - **Solutions**:\n",
    "     - **Check memory and context management**: Ensure that the memory management of the model is correctly implemented. For example, verify that the relevant parts of the context are updated and available during each interaction.\n",
    "     - **Limit context size**: If memory becomes too large or unstructured, the model may lose focus on the critical information. Limit the memory to the most relevant and recent exchanges.\n",
    "\n",
    "### 7. **Model Hyperparameters**:\n",
    "   - **Problem**: The model's parameters or temperature settings might not be properly configured for the task, causing it to produce irrelevant or verbose outputs.\n",
    "   - **Solutions**:\n",
    "     - **Adjust temperature and top-k settings**: Reduce the temperature for more deterministic and focused responses (e.g., temperature 0.3–0.5).\n",
    "     - **Set appropriate token limits**: Adjust the `max tokens` parameter to ensure that the model’s response remains concise and on-topic.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57918b30-aabb-473c-a547-64897b373d14",
   "metadata": {},
   "source": [
    "## The insights to the work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a9b0f-e0ba-47cc-a3f6-f97769e8de5d",
   "metadata": {},
   "source": [
    " * Here we work two datasets one is sourced from wikipedia about solar energy another dataset is our personal information which we have to build prompt for\n",
    " * We use two generators and two retrivers , one is from lama model , groq model using groq api we did proceed with that since we have package in compatability , installed the packages independently\n",
    " * The another one is the instruct-base which comes from tutorial notebook\n",
    " * The limitation to the task is to extract the irrelevat infomration or thresholding since we tried that but was not able to get the score\n",
    " * So this solution will give same results to the questions like \"What is solar energy?, what is solar panal?, what is solar system? with the instruct-base embeeding\"\n",
    " * Futher imporvements can be done using chain-of-thought or few shot learning for better reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81111439-79b1-4f14-b34c-866a812c7ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
